{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9ee1a4-997e-4193-9df2-711055c3c449",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "\n",
    "- Implement K fold cross validation\n",
    "- Understand Bootstrap Method and Bagging (bootstrap Aggregate)\n",
    "- Understand and implement Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c957703-fd51-40ee-9aff-d8ccf602d8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e41aa518-b2b0-4855-9baf-100edf9ec8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43056782, 0.79537749, 0.79221684, 0.78478276, 0.42021274,\n",
       "       0.68096881, 0.43256614, 0.71810031, 0.63858417, 0.3255384 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "model = DecisionTreeRegressor(max_depth = 3)\n",
    "cross_val_score(model, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1c161-d196-4daa-99be-f64c658c9dfe",
   "metadata": {},
   "source": [
    "#### Testing different models with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c800943-31ec-46e0-82f4-8e8c9bb3e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DecisionTreeRegressor(max_depth=None)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model2 = LinearRegression()\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "model3 = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d66f0cb6-f774-4bf6-91ba-d84d4ba56e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Regression Tree': 0.6779062591034359, 'Linear Regression': 0.6824999170803692, 'KNN': 0.415266722155892}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_pipeline = [model1, model2, model3]\n",
    "model_names = ['Regression Tree', 'Linear Regression', 'KNN']\n",
    "scores = {}\n",
    "i=0\n",
    "for model in model_pipeline:\n",
    "    mean_score = np.mean(cross_val_score(model, X_train, y_train, cv=10))\n",
    "#     mean_score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    scores[model_names[i]] = mean_score\n",
    "    i = i+1\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35415227-f72c-4938-9640-b54c02763f0e",
   "metadata": {},
   "source": [
    "#### Bootstraping\n",
    "\n",
    "- Bootstrap - To produce a reliable estimate we need enough samples in the dataset but sometimes it is not possible to collect enough real data. Bootstrap method allows us to emulate the process of obtaining new sample sets from the original data. Hence bootstrapping is the process of generating distinct data sets by repeatedly sampling observations (with replacement) from the original data set.\n",
    "- [link to the image - Generating Bootstrap Samples](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/7.08/7.08-bootstrapping.png)\n",
    "\n",
    "In the image above, we are generating `B` bootstrap samples from the original dataset. Since sampling is done with replacement, you would observe some repetition in the rows in some bootstrap samples. Each bootstrap sample is used to estimate alpha (for example which could be a measure of accuracy for a linear regression model). Then we take the mean of all alpha scores to obtain a more reliable final estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063e1b3-699f-437c-877d-47e11da522ed",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "- Why do we need bagging technique?\n",
    "\n",
    "  - One of the disadvantages with decision trees is that they have high variability in the result ie the results produced can vary greatly in their accuracy measures. This can be seen from the snapshot here: [link to the image - High Variance in Output for Regression Trees](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/7.08/7.08-high_variability.png)\n",
    "\n",
    "- Bagging is a general purpose technique that is used to reduce variance in a machine learning model. The idea is to use B Bootstrap samples and find the accuracy measure for each bootstrap sample. And then aggregate the results of all the bootstrap samples. This method is particularly useful for decision trees.\n",
    "\n",
    "- Bagging applied decision trees: B bootstrapped training sets are sampled from the original data. On each bootstrap sample, a decision tree is fit and a prediction is made. Then we average the resulting predictions. These trees are grown deep and have high variance. Averaging these B trees reduces the variance.\n",
    "\n",
    "- Essentially we are combining the results from hundreds or thousands of independently grown decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d46b4-633c-4227-9dd9-add462377b0c",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "- Random Forests are very similar to bagging except for one improvement over bagging method in terms of randomization of features chosen while building a tree for each bootstrap sample.\n",
    "\n",
    "- It also consists of building a large number of trees (a decision for each bootstrap sample).\n",
    "- For each decision tree for each bootstrap sample, instead of picking all the features while making the decision tree, only a random sample of `m` features are chosen from the total set of `p` predictors.\n",
    "- Hence the name random forests.\n",
    "- There is no rule of thumb/best value of `m` but usually m is chosen as the square root of `p` as a good starting point.\n",
    "- Using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5f74e-e94e-4ce0-9076-c730ff7a6258",
   "metadata": {},
   "source": [
    "#### Parameters in Random Forests\n",
    "\n",
    "- `n_estimators: int, default=100` - The number of trees in the forest.\n",
    "- `max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto”` - The number of features to consider when looking for the best split.\n",
    "- `bootstrapbool, default=True` - Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "- Some of these other parameters are the same ones as we have looked in decision trees.\n",
    "\n",
    "  - `criterion{“gini”, “entropy”}, default=”gini”`\n",
    "  - `max_depthint, default=None`\n",
    "  - `min_samples_splitint or float, default=2`\n",
    "  - `min_samples_leafint or float, default=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f67cb-c2bb-4605-b7b4-fb106046d697",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "- Using a random forest model on mail promotion data.\n",
    "- The first objective here is to make a classification model and predicting who are the customers that are more likely to respond.\n",
    "- The customers who are more likely to respond, on those predicted customers we will create a regression model to predict the amount of money they will donate.\n",
    "- It is important to note how we will retain the information from the column `TARGET_D` which is the target column for the regression model.\n",
    "- For the classification model now, we will use the cleaned data from the provided CSV files in the `files_for_lesson_and_activities` folder:\n",
    "  - `numerical.csv` has the numerical features (not normalized)\n",
    "  - `categorical.csv` has the categorical columns (not encoded)\n",
    "  - `target.csv` has the two target columns `TARGET_B` and `TARGET_D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4eccfc-af84-4380-9df1-89f53892fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "329b74ac-8289-457b-9ab9-20f2f5665502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    90569\n",
       "1     4843\n",
       "Name: TARGET_B, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical = pd.read_csv('./files_for_lesson_and_activities/numerical.csv')\n",
    "categorical = pd.read_csv('./files_for_lesson_and_activities/categorical.csv')\n",
    "targets = pd.read_csv('./files_for_lesson_and_activities/target.csv')\n",
    "data = pd.concat([numerical, categorical, targets], axis = 1)\n",
    "data['TARGET_B'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7af1837c-420d-44eb-8043-6d8b1440f183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4843, 339)\n"
     ]
    }
   ],
   "source": [
    "# Downsampling to balance data\n",
    "category_0 = data[data['TARGET_B']==0].sample(len(data[data['TARGET_B']==1]))\n",
    "print(category_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9916d21e-c244-4b3a-9cfd-ee8eab8a342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9686, 339)\n"
     ]
    }
   ],
   "source": [
    "category_1 = data[data['TARGET_B']== 1 ]\n",
    "data = pd.concat([category_0, category_1], axis = 0)\n",
    "data = data.sample(frac =1)\n",
    "data = data.reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0809d32f-bc30-48a5-aa52-614b38cced2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "y = data['TARGET_B']\n",
    "X = data.drop(['TARGET_B'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "375f5c89-f0c8-49f6-850a-8516fb1eb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalX = X.select_dtypes(np.number)\n",
    "categorcalX = X.select_dtypes(np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d714cc70-95b4-4a2d-a87d-3ad8512cd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first').fit(categorcalX)\n",
    "encoded_categorical = encoder.transform(categorcalX).toarray()\n",
    "encoded_categorical = pd.DataFrame(encoded_categorical)\n",
    "X = pd.concat([numericalX, encoded_categorical], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33d2cab9-47e1-45f3-b2de-a429893afe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09959f82-6828-4d6c-a24b-e4af320add58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retaining Info for Regression Model for Later\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# y_train_regression = X_train['TARGET_D']\n",
    "# y_test_regression = X_test['TARGET_D']\n",
    "\n",
    "# Now we can remove the column target d from the set of features\n",
    "X_train = X_train.drop(['TARGET_D'], axis = 1)\n",
    "X_test = X_test.drop(['TARGET_D'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16243c5d-6b22-4f77-ab65-37d8860b3d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4d0cad273391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             )\n\u001b[0;32m--> 304\u001b[0;31m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[1;32m    305\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    721\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# For cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "cross_val_scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "print(np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d174a82-d8c9-464c-bdc2-7ff571b2a8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
